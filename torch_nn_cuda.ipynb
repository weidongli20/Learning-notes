{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cc57e56397902256172cefdb104c66f090a94a0b7a8e28dfa1fd08ea4773cc44"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "#\n",
    "# Understanding PyTorch with an example: a step-by-step tutorial\n",
    "# Daniel Godoy\n",
    "#"
   ]
  },
  {
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "true_a, true_b = 1, 2\n",
    "y = true_a + true_b*x + 0.1*np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch tensors in CPU\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_tensor, y_tensor) # dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(y, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate a and b\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "model = ManualLinearRegression().to(device) \n",
    "# model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.8088]], device='cuda:0')), ('linear.bias', tensor([-0.4258], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] Training loss: 1.716\t Validation loss: 0.410\n",
      "[2] Training loss: 0.123\t Validation loss: 0.067\n",
      "[3] Training loss: 0.023\t Validation loss: 0.029\n",
      "[4] Training loss: 0.017\t Validation loss: 0.021\n",
      "[5] Training loss: 0.015\t Validation loss: 0.019\n",
      "[6] Training loss: 0.014\t Validation loss: 0.017\n",
      "[7] Training loss: 0.014\t Validation loss: 0.016\n",
      "[8] Training loss: 0.013\t Validation loss: 0.015\n",
      "[9] Training loss: 0.012\t Validation loss: 0.014\n",
      "[10] Training loss: 0.012\t Validation loss: 0.014\n",
      "[11] Training loss: 0.011\t Validation loss: 0.013\n",
      "[12] Training loss: 0.011\t Validation loss: 0.013\n",
      "[13] Training loss: 0.011\t Validation loss: 0.012\n",
      "[14] Training loss: 0.010\t Validation loss: 0.012\n",
      "[15] Training loss: 0.010\t Validation loss: 0.011\n",
      "[16] Training loss: 0.010\t Validation loss: 0.011\n",
      "[17] Training loss: 0.010\t Validation loss: 0.011\n",
      "[18] Training loss: 0.009\t Validation loss: 0.010\n",
      "[19] Training loss: 0.009\t Validation loss: 0.010\n",
      "[20] Training loss: 0.009\t Validation loss: 0.010\n",
      "[21] Training loss: 0.009\t Validation loss: 0.010\n",
      "[22] Training loss: 0.009\t Validation loss: 0.010\n",
      "[23] Training loss: 0.009\t Validation loss: 0.009\n",
      "[24] Training loss: 0.009\t Validation loss: 0.009\n",
      "[25] Training loss: 0.009\t Validation loss: 0.009\n",
      "[26] Training loss: 0.009\t Validation loss: 0.009\n",
      "[27] Training loss: 0.009\t Validation loss: 0.009\n",
      "[28] Training loss: 0.009\t Validation loss: 0.009\n",
      "[29] Training loss: 0.009\t Validation loss: 0.009\n",
      "[30] Training loss: 0.009\t Validation loss: 0.009\n",
      "[31] Training loss: 0.009\t Validation loss: 0.009\n",
      "[32] Training loss: 0.009\t Validation loss: 0.009\n",
      "[33] Training loss: 0.009\t Validation loss: 0.009\n",
      "[34] Training loss: 0.009\t Validation loss: 0.009\n",
      "[35] Training loss: 0.009\t Validation loss: 0.008\n",
      "[36] Training loss: 0.009\t Validation loss: 0.008\n",
      "[37] Training loss: 0.008\t Validation loss: 0.008\n",
      "[38] Training loss: 0.008\t Validation loss: 0.008\n",
      "[39] Training loss: 0.008\t Validation loss: 0.008\n",
      "[40] Training loss: 0.008\t Validation loss: 0.008\n",
      "[41] Training loss: 0.008\t Validation loss: 0.008\n",
      "[42] Training loss: 0.008\t Validation loss: 0.008\n",
      "[43] Training loss: 0.008\t Validation loss: 0.008\n",
      "[44] Training loss: 0.008\t Validation loss: 0.008\n",
      "[45] Training loss: 0.008\t Validation loss: 0.008\n",
      "[46] Training loss: 0.008\t Validation loss: 0.008\n",
      "[47] Training loss: 0.008\t Validation loss: 0.008\n",
      "[48] Training loss: 0.008\t Validation loss: 0.008\n",
      "[49] Training loss: 0.008\t Validation loss: 0.008\n",
      "[50] Training loss: 0.008\t Validation loss: 0.008\n",
      "[51] Training loss: 0.008\t Validation loss: 0.008\n",
      "[52] Training loss: 0.008\t Validation loss: 0.008\n",
      "[53] Training loss: 0.008\t Validation loss: 0.008\n",
      "[54] Training loss: 0.008\t Validation loss: 0.008\n",
      "[55] Training loss: 0.008\t Validation loss: 0.008\n",
      "[56] Training loss: 0.008\t Validation loss: 0.008\n",
      "[57] Training loss: 0.008\t Validation loss: 0.008\n",
      "[58] Training loss: 0.008\t Validation loss: 0.008\n",
      "[59] Training loss: 0.008\t Validation loss: 0.008\n",
      "[60] Training loss: 0.008\t Validation loss: 0.008\n",
      "[61] Training loss: 0.008\t Validation loss: 0.008\n",
      "[62] Training loss: 0.008\t Validation loss: 0.008\n",
      "[63] Training loss: 0.008\t Validation loss: 0.008\n",
      "[64] Training loss: 0.008\t Validation loss: 0.008\n",
      "[65] Training loss: 0.008\t Validation loss: 0.008\n",
      "[66] Training loss: 0.008\t Validation loss: 0.008\n",
      "[67] Training loss: 0.008\t Validation loss: 0.008\n",
      "[68] Training loss: 0.008\t Validation loss: 0.008\n",
      "[69] Training loss: 0.008\t Validation loss: 0.008\n",
      "[70] Training loss: 0.008\t Validation loss: 0.008\n",
      "[71] Training loss: 0.008\t Validation loss: 0.008\n",
      "[72] Training loss: 0.008\t Validation loss: 0.008\n",
      "[73] Training loss: 0.008\t Validation loss: 0.008\n",
      "[74] Training loss: 0.008\t Validation loss: 0.008\n",
      "[75] Training loss: 0.008\t Validation loss: 0.008\n",
      "[76] Training loss: 0.008\t Validation loss: 0.008\n",
      "[77] Training loss: 0.008\t Validation loss: 0.008\n",
      "[78] Training loss: 0.008\t Validation loss: 0.008\n",
      "[79] Training loss: 0.008\t Validation loss: 0.008\n",
      "[80] Training loss: 0.008\t Validation loss: 0.008\n",
      "[81] Training loss: 0.008\t Validation loss: 0.008\n",
      "[82] Training loss: 0.008\t Validation loss: 0.008\n",
      "[83] Training loss: 0.008\t Validation loss: 0.008\n",
      "[84] Training loss: 0.008\t Validation loss: 0.008\n",
      "[85] Training loss: 0.008\t Validation loss: 0.008\n",
      "[86] Training loss: 0.008\t Validation loss: 0.008\n",
      "[87] Training loss: 0.008\t Validation loss: 0.008\n",
      "[88] Training loss: 0.008\t Validation loss: 0.008\n",
      "[89] Training loss: 0.008\t Validation loss: 0.008\n",
      "[90] Training loss: 0.008\t Validation loss: 0.008\n",
      "[91] Training loss: 0.008\t Validation loss: 0.008\n",
      "[92] Training loss: 0.008\t Validation loss: 0.008\n",
      "[93] Training loss: 0.008\t Validation loss: 0.008\n",
      "[94] Training loss: 0.008\t Validation loss: 0.008\n",
      "[95] Training loss: 0.008\t Validation loss: 0.008\n",
      "[96] Training loss: 0.008\t Validation loss: 0.008\n",
      "[97] Training loss: 0.008\t Validation loss: 0.008\n",
      "[98] Training loss: 0.008\t Validation loss: 0.008\n",
      "[99] Training loss: 0.008\t Validation loss: 0.008\n",
      "[100] Training loss: 0.008\t Validation loss: 0.008\n",
      "OrderedDict([('linear.weight', tensor([[1.9378]], device='cuda:0')), ('linear.bias', tensor([1.0182], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat).item()\n",
    "            val_losses.append(val_loss)\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f82606cf790>,\n",
       " <matplotlib.lines.Line2D at 0x7f8247a93c50>]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m558f150dee\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.17141\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(106.80891 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.659013\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(168.296513 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"236.146617\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(229.784117 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.63422\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(291.27172 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.121823\" xlink:href=\"#m558f150dee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(349.578073 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m27b2fbd10b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"215.676581\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.00 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 219.4758)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"186.745671\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.25 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 190.544889)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"157.81476\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 161.613979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"128.88385\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 132.683069)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"99.95294\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 103.752159)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"71.02203\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 74.821248)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"42.091119\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 45.890338)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m27b2fbd10b\" y=\"13.160209\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 16.959428)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p11233fee06)\" d=\"M 51.683807 17.083636 \nL 54.758187 201.500383 \nL 57.832567 212.972422 \nL 60.906947 213.758801 \nL 63.981327 213.90641 \nL 67.055708 214.010671 \nL 70.130088 214.101908 \nL 73.204468 214.18163 \nL 76.278848 214.250935 \nL 79.353228 214.311079 \nL 82.427608 214.363245 \nL 85.501989 214.40848 \nL 88.576369 214.447701 \nL 91.650749 214.481704 \nL 94.725129 214.51118 \nL 97.799509 214.536729 \nL 100.873889 214.558872 \nL 103.94827 214.578059 \nL 107.02265 214.594684 \nL 110.09703 214.609086 \nL 113.17141 214.621561 \nL 116.24579 214.632365 \nL 119.32017 214.641719 \nL 122.394551 214.649817 \nL 125.468931 214.656825 \nL 128.543311 214.662889 \nL 131.617691 214.668136 \nL 134.692071 214.672673 \nL 137.766451 214.676595 \nL 140.840832 214.679986 \nL 143.915212 214.682915 \nL 146.989592 214.685445 \nL 150.063972 214.68763 \nL 153.138352 214.689515 \nL 156.212732 214.69114 \nL 159.287113 214.692542 \nL 162.361493 214.693749 \nL 165.435873 214.69479 \nL 168.510253 214.695684 \nL 171.584633 214.696454 \nL 174.659013 214.697115 \nL 177.733394 214.697682 \nL 180.807774 214.698168 \nL 183.882154 214.698585 \nL 186.956534 214.698942 \nL 190.030914 214.699247 \nL 193.105294 214.699507 \nL 196.179675 214.699728 \nL 199.254055 214.699917 \nL 202.328435 214.700077 \nL 205.402815 214.700212 \nL 208.477195 214.700326 \nL 211.551575 214.700423 \nL 214.625956 214.700503 \nL 217.700336 214.700571 \nL 220.774716 214.700627 \nL 223.849096 214.700674 \nL 226.923476 214.700713 \nL 229.997856 214.700744 \nL 233.072237 214.70077 \nL 236.146617 214.70079 \nL 239.220997 214.700806 \nL 242.295377 214.700819 \nL 245.369757 214.700828 \nL 248.444137 214.700835 \nL 251.518518 214.70084 \nL 254.592898 214.700843 \nL 257.667278 214.700845 \nL 260.741658 214.700846 \nL 263.816038 214.700845 \nL 266.890418 214.700844 \nL 269.964799 214.700842 \nL 273.039179 214.70084 \nL 276.113559 214.700837 \nL 279.187939 214.700835 \nL 282.262319 214.700831 \nL 285.336699 214.700828 \nL 288.41108 214.700825 \nL 291.48546 214.700822 \nL 294.55984 214.700818 \nL 297.63422 214.700815 \nL 300.7086 214.700812 \nL 303.78298 214.700808 \nL 306.857361 214.700805 \nL 309.931741 214.700802 \nL 313.006121 214.7008 \nL 316.080501 214.700797 \nL 319.154881 214.700794 \nL 322.229261 214.700791 \nL 325.303642 214.700789 \nL 328.378022 214.700787 \nL 331.452402 214.700784 \nL 334.526782 214.700782 \nL 337.601162 214.70078 \nL 340.675542 214.700778 \nL 343.749923 214.700776 \nL 346.824303 214.700775 \nL 349.898683 214.700774 \nL 352.973063 214.700772 \nL 356.047443 214.700771 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p11233fee06)\" d=\"M 51.683807 168.194831 \nL 54.758187 207.938655 \nL 57.832567 212.362967 \nL 60.906947 213.223392 \nL 63.981327 213.519726 \nL 67.055708 213.685858 \nL 70.130088 213.810244 \nL 73.204468 213.914649 \nL 76.278848 214.005533 \nL 79.353228 214.085529 \nL 82.427608 214.156208 \nL 85.501989 214.21877 \nL 88.576369 214.274214 \nL 91.650749 214.323403 \nL 94.725129 214.367094 \nL 97.799509 214.405945 \nL 100.873889 214.440527 \nL 103.94827 214.471346 \nL 107.02265 214.498844 \nL 110.09703 214.523411 \nL 113.17141 214.545382 \nL 116.24579 214.565058 \nL 119.32017 214.582701 \nL 122.394551 214.598538 \nL 125.468931 214.612774 \nL 128.543311 214.625586 \nL 131.617691 214.63713 \nL 134.692071 214.647548 \nL 137.766451 214.656956 \nL 140.840832 214.665465 \nL 143.915212 214.67317 \nL 146.989592 214.680157 \nL 150.063972 214.686497 \nL 153.138352 214.69226 \nL 156.212732 214.697504 \nL 159.287113 214.702278 \nL 162.361493 214.706632 \nL 165.435873 214.710605 \nL 168.510253 214.714236 \nL 171.584633 214.717557 \nL 174.659013 214.720596 \nL 177.733394 214.723379 \nL 180.807774 214.725932 \nL 183.882154 214.728277 \nL 186.956534 214.73043 \nL 190.030914 214.73241 \nL 193.105294 214.734231 \nL 196.179675 214.735905 \nL 199.254055 214.737452 \nL 202.328435 214.738876 \nL 205.402815 214.740189 \nL 208.477195 214.741401 \nL 211.551575 214.742521 \nL 214.625956 214.743556 \nL 217.700336 214.744512 \nL 220.774716 214.745398 \nL 223.849096 214.746218 \nL 226.923476 214.746975 \nL 229.997856 214.747675 \nL 233.072237 214.748327 \nL 236.146617 214.74893 \nL 239.220997 214.749488 \nL 242.295377 214.750005 \nL 245.369757 214.750486 \nL 248.444137 214.750934 \nL 251.518518 214.751348 \nL 254.592898 214.751733 \nL 257.667278 214.752091 \nL 260.741658 214.752422 \nL 263.816038 214.752729 \nL 266.890418 214.753015 \nL 269.964799 214.753283 \nL 273.039179 214.753528 \nL 276.113559 214.75376 \nL 279.187939 214.753973 \nL 282.262319 214.754172 \nL 285.336699 214.754357 \nL 288.41108 214.754529 \nL 291.48546 214.75469 \nL 294.55984 214.754836 \nL 297.63422 214.754977 \nL 300.7086 214.755105 \nL 303.78298 214.755227 \nL 306.857361 214.755337 \nL 309.931741 214.75544 \nL 313.006121 214.755537 \nL 316.080501 214.755626 \nL 319.154881 214.755711 \nL 322.229261 214.75579 \nL 325.303642 214.755862 \nL 328.378022 214.75593 \nL 331.452402 214.755992 \nL 334.526782 214.756052 \nL 337.601162 214.756106 \nL 340.675542 214.756157 \nL 343.749923 214.756204 \nL 346.824303 214.756248 \nL 349.898683 214.756288 \nL 352.973063 214.756327 \nL 356.047443 214.756364 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p11233fee06\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.arange(len(training_losses)), training_losses, \"-b\", \n",
    "         np.arange(len(validation_losses)), validation_losses, \"-r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}