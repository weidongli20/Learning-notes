{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "# Data Generation\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(100,1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# inital start values\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "lr = 1e-1\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epoches\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Compute our model's predicated output\n",
    "    yhat = a + b * x_train\n",
    "\n",
    "    # How wrong is our model? That's the error\n",
    "    # It is a regression, so it computes mean \n",
    "    # squared error (MSE)    \n",
    "    error = (y_train - yhat)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Computes gradient for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "\n",
    "    # Updates parameters using gradients and learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Sanity check: do we get the same results as our \n",
    " # gradient descent?\n",
    " from sklearn.linear_model import LinearRegression\n",
    " linr = LinearRegression()\n",
    " linr.fit(x_train, y_train)\n",
    " print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scalar (a single number) has zero dimensions, \n",
    "# a vector has one dimension, \n",
    "# a matrix has two dimensions and \n",
    "# a tensor has three or more dimensions. That’s it!\n",
    "# But, to keep things simple, it is commonplace to \n",
    "# call vectors and matrices tensors as well — so, \n",
    "# from now on, everything is either a scalar or a tensor.\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "# from torchviz import make_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data was in numpy arrays, but we need to transform\n",
    "# them to pytorch's tensors and then we send them to\n",
    "# the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is\n",
    "# more useful since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First\n",
    "# Initialized parameters \"a\" and \"b\" randomly, ALMOST as we did\n",
    "# in numpy since we want to apply gradient descent to these \n",
    "# parameters, \n",
    "# we need to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "\n",
    "# Second\n",
    "# But what is we want to run it on a GPU? We could just send them\n",
    "# to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(a, b)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient\n",
    "\n",
    "# Third\n",
    "# We can either create regular tensors and send them to the device\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients ...device\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "# In PyTorch, every method that ends with an underscore (_) makes \n",
    "# changes in-place, meaning, they will modify the underlying variable.\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although the last approach worked fine, it is much better to \n",
    "# assign tensors to a device at the moment of their creation.\n",
    "# RECOMMENDED!\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autograd\n",
    "# Autograd is PyTorch’s automatic differentiation package. \n",
    "# Thanks to it, \n",
    "# we don’t need to worry about partial derivatives, \n",
    "# chain rule or anything like it.\n",
    "# So, how do we tell PyTorch to do its thing and compute \n",
    "# all gradients? \n",
    "# That’s what backward() is good for.\n",
    "\n",
    "# Do you remember the starting point for computing the gradients? \n",
    "# It was the loss, \n",
    "# as we computed its partial derivatives w.r.t. our parameters. \n",
    "# Hence, we need to invoke the backward() method from \n",
    "# the corresponding Python variable, \n",
    "# like, loss.backward().\n",
    "\n",
    "# What about the actual values of the gradients? \n",
    "# We can inspect them by \n",
    "# looking at the grad attribute of a tensor.\n",
    "\n",
    "# If you check the method’s documentation, it clearly states \n",
    "# that gradients are accumulated. \n",
    "# So, every time we use the gradients to update the parameters, \n",
    "# we need to zero the gradients afterwards. And that’s what \n",
    "# zero_() is good for.\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "# sets the seed for generating random numbers\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more mannual computation of gradients\n",
    "    # We just tell PyTorch to work its way BACKWARDs from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients ...\n",
    "    #print(a.grad)\n",
    "    #print(b.grad)\n",
    "\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient \n",
    "    # computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch \n",
    "    # uses ...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell\n",
    "    # it to let it go ... \n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, we’ve been manually updating the parameters using the \n",
    "# computed gradients. \n",
    "# That’s probably fine for two parameters… but what if we had \n",
    "# a whole lot of them?! \n",
    "# We use one of PyTorch’s optimizers, like SGD or Adam.\n",
    "# An optimizer takes the parameters we want to update, \n",
    "# the learning rate \n",
    "# we want to use (and possibly many other hyper-parameters as well!) \n",
    "# and \n",
    "# performs the updates through its step() method\n",
    "# Besides, we also don’t need to zero the gradients one by one anymore. \n",
    "# We just invoke the optimizer’s zero_grad() method and that’s it!\n",
    "# \n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "# Define a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #   a -= lr * a.grad\n",
    "    #   b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "\n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "# We now tackle the loss computation. As expected, PyTorch got us covered once again. \n",
    "# There are many loss functions to choose from, depending on the task at hand. \n",
    "# Since ours is a regression, we are using the Mean Square Error (MSE) loss.\n",
    "# Notice that nn.MSELoss actually creates a loss function for us — \n",
    "# it is NOT the loss function itself. Moreover, you can specify a reduction \n",
    "# method to be applied, \n",
    "# that is, how do you want to aggregate the results for individual points — \n",
    "# you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "print(a, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "    # No more manual loss!\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# In PyTorch, a model is represented by a regular Python class that \n",
    "# inherits from the Module class.\n",
    "# The most fundamental methods it needs to implement are:\n",
    "# __init__(self): it defines the parts that make up the model —in our case, \n",
    "# two parameters, a and b.\n",
    "# You are not limited to defining parameters, though… models can contain \n",
    "# other models (or layers) \n",
    "# as its attributes as well, so you can easily nest them. We’ll see \n",
    "# an example of this shortly as well.\n",
    "# forward(self, x): it performs the actual computation, that is, \n",
    "# it outputs a prediction, given the input x.\n",
    "# You should NOT call the forward(x) method, though. You should call \n",
    "# the whole model itself, \n",
    "# as in model(x) to perform a forward pass and output predictions.\n",
    "\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to\n",
    "        # wrap them with nn.Parameter\n",
    "        # to tell PyTorch these tensors should be considered parameters of \n",
    "        # the model they are an attribute of.\n",
    "        # Why should we care about that? By doing so, we can use our model’s \n",
    "        # parameters() method to retrieve an iterator over all model’s parameters, \n",
    "        # even those parameters of nested models, that we can use to \n",
    "        # feed our optimizer (instead of building a list of parameters ourselves!).\n",
    "        # Moreover, we can get the current values for all parameters \n",
    "        # using our model’s state_dict() method.\n",
    "        self.a = nn.Parameter(torch.randn(1, \n",
    "                                requires_grad=True, \n",
    "                                dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, \n",
    "                                requires_grad=True, \n",
    "                                dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Computes teh outputs / predications\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: we need to send our model to the same device where the data is. \n",
    "# If our data is made of GPU tensors, our model must “live” inside the GPU as well.\n",
    "# We can use all these handy methods to change our code, which should be \n",
    "# looking like this:\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can created a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # In PyTorch, models have a train() method which, somewhat disappointingly, \n",
    "    # does NOT perform a training step. Its only purpose is to set the model to \n",
    "    # training mode. Why is this important? Some models may use mechanisms like Dropout, \n",
    "    # for instance, which have distinct behaviors in training and evaluation phases.\n",
    "    model.train()\n",
    "    # No more manual predication!\n",
    "    #  yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our model, we manually created two parameters to perform a linear regression. \n",
    "# Let’s use PyTorch’s Linear model as an attribute of our own, thus creating \n",
    "# a nested model.\n",
    "\n",
    "# In the __init__ method, we created an attribute that contains \n",
    "# our nested Linear model.\n",
    "# In the forward() method, we call the nested model itself to \n",
    "# perform the forward pass \n",
    "# (notice, we are not calling self.linear.forward(x)!).\n",
    "\n",
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single\n",
    "        # input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predications\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Now we can created a model and send it at once to the device\n",
    "model = LayerLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    yhat = model(x_train_tensor)\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    yhat = model(x_train_tensor)\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Step\n",
    "# So far, we’ve defined an optimizer, a loss function and a model. \n",
    "# Scroll up a bit and take a quick look at the code inside the loop. \n",
    "# Would it change if we were using a different optimizer, or loss, \n",
    "# or even model? \n",
    "# If not, how can we make it more generic?\n",
    "# Well, I guess we could say all these lines of code perform a training \n",
    "# step, \n",
    "# given those three elements (optimizer, loss and model),the features \n",
    "# and the labels.\n",
    "# So, how about writing a function that takes those three elements and \n",
    "# returns another function that performs a training step, taking a set \n",
    "# of \n",
    "# features and labels as arguments and returning the corresponding loss?\n",
    "# Then we can use this general-purpose function to build a train_step() \n",
    "# function \n",
    "# to be called inside our training loop. Now our code should look \n",
    "# like this… \n",
    "# see how tiny the training loop is now?\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeros gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Return the loss\n",
    "        return loss.item()\n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch ... \n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss \n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyThorch Dataset\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "# if we want to go serious about all this, we must use mini-batch gradient descent. \n",
    "# Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate a and b\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "#model = ManualLinearRegression().to(device) \n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch’s random_split() method is an easy and familiar way of performing \n",
    "# a training-validation split. Just keep in mind that, in our example,\n",
    "# we need to apply it to the whole dataset (not the training dataset \n",
    "# we built in two sections ago).\n",
    "\n",
    "from torch.utils.data.dataset import random_split \n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# are two small, yet important, things to consider:\n",
    "# torch.no_grad(): even though it won’t make a difference in our \n",
    "# simple model, \n",
    "# it is a good practice to wrap the validation inner loop with \n",
    "# this context manager \n",
    "# to disable any gradient calculation that you may inadvertently \n",
    "# trigger — \n",
    "# gradients belong in training, not in validation steps;\n",
    "# eval(): the only thing it does is setting the model to evaluation \n",
    "# mode \n",
    "# (just like its train() counterpart did), so the model can adjust \n",
    "# its behavior regarding some operations, like Dropout.\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    training_losses.append(training_loss)\n",
    "    #losses.append(loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "        #val_losses.append(val_loss.item())\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.arange(len(training_losses)), training_losses, \"-b\", \n",
    "         np.arange(len(validation_losses)), validation_losses, \"-r\")\n",
    "#plt.plot(np.arange(len(losses)), losses, \"-b\", \n",
    "#         np.arange(len(val_losses)), val_losses, \"-r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a253be52ec3184b7a370399d56db168d13f93ee115782822238af750eed14bf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}